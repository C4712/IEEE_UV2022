{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensor\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, sampler\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib\n",
    "import sys\n",
    "import timm\n",
    "import albumentations as A\n",
    "from mmdet.apis import inference_detector, init_detector\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "sys.path.append('./ensemble_boxes-1.0.4')\n",
    "from ensemble_boxes import *\n",
    "matplotlib.use('TkAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 16:08:44,496 - mmcls - INFO - backbone out_indices: (1, 2, 3, 4)\n",
      "2022-10-15 16:08:44,496 - mmcls - INFO - backbone out_channels: [48, 64, 160, 256]\n",
      "2022-10-15 16:08:44,497 - mmcls - INFO - backbone out_strides: [4, 8, 16, 32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: /home/liuyun/ali/vma/mmdetection-master/save_models/heavy_alb_enhance_mosaic/fold0/best_bbox_mAP_epoch_41.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 430/430 [02:36<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "def run_nms(bboxes, confs,classs, image_size, iou_thr=0.50, skip_box_thr=0.0001, weights=None):\n",
    "    boxes =  [bbox/(image_size-1) for bbox in bboxes]\n",
    "    scores = [conf for conf in confs]    \n",
    "    labels = [np.ones(conf.shape[0]) for conf in confs]\n",
    "    boxes, scores, labels = nms(boxes, scores, labels, weights=weights, iou_thr=iou_thr)\n",
    "    boxes = boxes*(image_size-1)\n",
    "    return boxes, scores, labels\n",
    "\n",
    "def run_nmw(bboxes, confs,classs, image_size, iou_thr=0.50, skip_box_thr=0.0001, weights=None):\n",
    "    boxes =  [bbox/(image_size-1) for bbox in bboxes]\n",
    "    scores = [conf for conf in confs]    \n",
    "    labels = [np.ones(conf.shape[0]) for conf in confs]\n",
    "    boxes, scores, labels = non_maximum_weighted(boxes, scores, labels, weights=weights, iou_thr=iou_thr)\n",
    "    boxes = boxes*(image_size-1)\n",
    "    return boxes, scores, labels\n",
    "\n",
    "\n",
    "def run_soft_nms(bboxes, confs,classs, image_size, iou_thr=0.50, skip_box_thr=0.0001,sigma=0.1, weights=None):\n",
    "    boxes =  [bbox/(image_size-1) for bbox in bboxes]\n",
    "    scores = [conf for conf in confs]    \n",
    "    labels = [np.ones(conf.shape[0]) for conf in confs]\n",
    "    boxes, scores, labels = soft_nms(boxes, scores, labels, weights=weights, iou_thr=iou_thr, sigma=sigma, thresh=skip_box_thr)\n",
    "    boxes = boxes*(image_size-1)\n",
    "    return boxes, scores, labels\n",
    "\n",
    "def run_wbf(bboxes, confs,classs, iou_thr=0.50, skip_box_thr=0.5, weights=None):\n",
    "    boxes =  [bbox for bbox in bboxes]\n",
    "    scores = [conf for conf in confs]    \n",
    "    labels = [cls for cls in classs]\n",
    "    #print(labels)\n",
    "    #labels = convert_same(labels)\n",
    "    #print(labels)\n",
    "    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    #boxes = boxes*(image_size-1)\n",
    "    return boxes, scores, labels\n",
    "def load_model(ckpt_path, conf=0.25, iou=0.50):\n",
    "    model = torch.hub.load('/home/liuyun/ali/vma/hist_log/focal_m_1280',\n",
    "                           'custom',\n",
    "                           path=ckpt_path,\n",
    "                           source='local',\n",
    "                           force_reload=True)  # local repo\n",
    "    model.conf = conf  # NMS confidence threshold\n",
    "    model.iou  = iou  # NMS IoU threshold\n",
    "    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n",
    "    model.multi_label = False  # NMS multiple labels per box\n",
    "    #model.agnostic = True\n",
    "    model.max_det = 1000  # maximum number of detections per image\n",
    "    return model\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model('tf_efficientnetv2_s', pretrained=pretrained, in_chans=3)\n",
    "        #self.model.load_state_dict(torch.load('../data/tf_efficientnetv2_s-eb54923e.pth'))\n",
    "        self.n_features = self.model.classifier.in_features\n",
    "        self.model.global_pool = nn.Identity()\n",
    "        self.model.classifier = nn.Identity()\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Linear(self.n_features, 8)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        bs = x.size(0)\n",
    "        features = self.model(x)\n",
    "        pool_feature = self.pooling(features).view(bs, -1)\n",
    "        output = self.classifier(pool_feature)\n",
    "        return output\n",
    "def clasifier_part(input_img,det_result,cls_models):\n",
    "    transforms = A.Compose([\n",
    "            A.Resize(128, 128),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    img = input_img[int(det_result[1]):int(det_result[3]),int(det_result[0]):int(det_result[2]),:]\n",
    "    #cv2.imwrite('testpart.png',img)\n",
    "    img = transforms(image=img)['image'] / 255.0\n",
    "    img = img.to(device)\n",
    "    final_out = None\n",
    "    with torch.no_grad():\n",
    "        for model in cls_models:\n",
    "            out = F.softmax(model(img.unsqueeze(0)),dim=1)\n",
    "            if final_out is not None:\n",
    "                final_out += out\n",
    "            else:\n",
    "                final_out = out\n",
    "    final_out /= len(cls_models)\n",
    "    final_pred = torch.argmax(final_out)\n",
    "    return int(final_pred)\n",
    "def predict(model, img, size=1280, augment=False):\n",
    "    height, width = img.shape[:2]\n",
    "    #cv2.imwrite('./test.png',img)\n",
    "    #img = np.ascontiguousarray(img)\n",
    "    with torch.no_grad():\n",
    "        results = model(img, size=size, augment=augment)  # custom inference size\n",
    "    preds = results.pandas().xyxyn[0]\n",
    "    #bboxes = preds[['xcenter', 'ycenter', 'width', 'height']].values\n",
    "    bboxes = preds[['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "    #x1 = int(bboxes[0][0] - bboxes[0][2] // 2)\n",
    "    #y1 = int(bboxes[0][1] - bboxes[0][3] // 2)\n",
    "    #x2 = int(bboxes[0][0] + bboxes[0][2] // 2)\n",
    "    #y2 = int(bboxes[0][1] + bboxes[0][3] // 2)\n",
    "    #cv2.rectangle(img,(x1,y1),(x2,y2),(255, 0, 0), 1)\n",
    "    #cv2.imwrite('./test_box.png',img)\n",
    "    if len(bboxes):\n",
    "        bboxes = bboxes#voc2coco(bboxes, height, width).astype(int)\n",
    "        confs = preds.confidence.values\n",
    "        classes = preds['class'].values\n",
    "        return bboxes.tolist(), list(confs),list(classes)\n",
    "    else:\n",
    "        return [], [],[]\n",
    "\n",
    "\n",
    "def predict_mmdet(model,path):\n",
    "    img = cv2.imread(path)\n",
    "    w,h = img.shape[:2]\n",
    "    result = inference_detector(model, path)\n",
    "    bbox_result = result\n",
    "    bboxes = np.vstack(bbox_result)\n",
    "    confis = [bboxes[i][-1] for i in range(0, len(bboxes))]\n",
    "    classes = [\n",
    "        np.full(bbox.shape[0], i, dtype=np.int32)\n",
    "        for i, bbox in enumerate(bbox_result)\n",
    "    ]\n",
    "    classes = np.concatenate(classes)\n",
    "    only_boxes = bboxes[:,:4]\n",
    "   \n",
    "    only_boxes[:,0] /= h\n",
    "    only_boxes[:,1] /= w\n",
    "    only_boxes[:,2] /= h\n",
    "    only_boxes[:,3] /= w\n",
    "    \n",
    "    if len(confis) == 0:\n",
    "        return [],[],[]\n",
    "    else:\n",
    "        return only_boxes.tolist(),confis,classes.tolist()\n",
    "def convert_same(classes):\n",
    "    vote_num =  np.zeros((5))\n",
    "    lenth = 0\n",
    "    total_classes = []\n",
    "    for m_classes in classes:\n",
    "        lenth += len(m_classes)\n",
    "        total_classes += m_classes\n",
    "    if 5 in total_classes or 6 in total_classes or 7 in total_classes or lenth <= 10:\n",
    "        return classes\n",
    "    else:\n",
    "        for cls in total_classes:\n",
    "            vote_num[cls] += 1\n",
    "    max_idx = int(np.where(vote_num == np.max(vote_num))[0][0])\n",
    "    for i,m_cls in enumerate(classes):\n",
    "        for j,cur_cls in enumerate(m_cls):\n",
    "            classes[i][j] = max_idx\n",
    "    \n",
    "    return classes\n",
    "\n",
    "\n",
    "def main():\n",
    "    #cls model\n",
    "    model_cls_0 = CustomModel()\n",
    "    model_cls_0.load_state_dict(torch.load(f'/home/liuyun/ali/vma/hist_weights/log1013/tf_efficientnetv2_s_fold0_best_score.pth',map_location=torch.device('cpu'))['model'])\n",
    "    model_cls_0.to(device)\n",
    "    model_cls_0.eval()\n",
    "    model_cls_1 = CustomModel()\n",
    "    model_cls_1.load_state_dict(torch.load(f'/home/liuyun/ali/vma/hist_weights/log1013/tf_efficientnetv2_s_fold1_best_score.pth',map_location=torch.device('cpu'))['model'])\n",
    "    model_cls_1.to(device)\n",
    "    model_cls_1.eval()\n",
    "    model_cls_2 = CustomModel()\n",
    "    model_cls_2.load_state_dict(torch.load(f'/home/liuyun/ali/vma/hist_weights/log1013/tf_efficientnetv2_s_fold2_best_score.pth',map_location=torch.device('cpu'))['model'])\n",
    "    model_cls_2.to(device)\n",
    "    model_cls_2.eval()\n",
    "    model_cls_3 = CustomModel()\n",
    "    model_cls_3.load_state_dict(torch.load(f'/home/liuyun/ali/vma/hist_weights/log1013/tf_efficientnetv2_s_fold3_best_score.pth',map_location=torch.device('cpu'))['model'])\n",
    "    model_cls_3.to(device)\n",
    "    model_cls_3.eval()\n",
    "    model_cls_4 = CustomModel()\n",
    "    model_cls_4.load_state_dict(torch.load(f'/home/liuyun/ali/vma/hist_weights/log1013/tf_efficientnetv2_s_fold4_best_score.pth',map_location=torch.device('cpu'))['model'])\n",
    "    model_cls_4.to(device)\n",
    "    model_cls_4.eval()\n",
    "    \n",
    "    model_cls = [model_cls_0,model_cls_1,model_cls_2,model_cls_3,model_cls_4]\n",
    "    labels = []\n",
    "    #yolo model\n",
    "    CKPT_PATH_0 = '/home/liuyun/ali/vma/hist_log/focal_m_1280/runs/log1009/baseline_fold0/weights/best.pt'\n",
    "    CKPT_PATH_1 = '/home/liuyun/ali/vma/hist_log/focal_m_1280/runs/log1009/baseline_fold1/weights/best.pt'\n",
    "    CKPT_PATH_2 = '/home/liuyun/ali/vma/hist_log/focal_m_1280/runs/log1009/baseline_fold2/weights/best.pt'\n",
    "    CKPT_PATH_3 = '/home/liuyun/ali/vma/hist_log/focal_m_1280/runs/log1009/baseline_fold3/weights/best.pt'\n",
    "    CKPT_PATH_4 = '/home/liuyun/ali/vma/hist_log/focal_m_1280/runs/log1009/baseline_fold4/weights/best.pt'\n",
    "    Test_files = os.listdir('../data/test_images/images')\n",
    "    IMG_SIZE = 1280\n",
    "    AUGMENT = True\n",
    "    #model_0 = load_model(CKPT_PATH_0, conf=0.27, iou=0.5)\n",
    "    #model_1 = load_model(CKPT_PATH_1, conf=0.61, iou=0.5)\n",
    "    #model_2 = load_model(CKPT_PATH_2, conf=0.4, iou=0.5)\n",
    "    #model_3 = load_model(CKPT_PATH_3, conf=0.55, iou=0.5)\n",
    "    #model_4 = load_model(CKPT_PATH_4, conf=0.6, iou=0.5)\n",
    "    #mmdet cascade rcnn model\n",
    "    # config文件\n",
    "    config_file = '/home/liuyun/ali/vma/mmdetection-master/save_models/heavy_alb_enhance_mosaic/fold0/cascade_rcnn_coco_alb_mix.py'\n",
    "    checkpoint_file = '/home/liuyun/ali/vma/mmdetection-master/save_models/heavy_alb_enhance_mosaic/fold0/best_bbox_mAP_epoch_41.pth'#'./checkpoint/best_bbox_mAP_epoch_146.pth'\n",
    "    model_cascade_0 = init_detector(config_file, checkpoint_file, device='cuda:0')\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for idx, name in tqdm(enumerate(Test_files),total=len(Test_files)):\n",
    "        cur_name = f'test{idx}'#name.split('.')[0]\n",
    "        file_path = f'../data/test_images/images/test{idx}.png'\n",
    "        #yolo inference\n",
    "        img = cv2.imread(file_path)[...,::-1]\n",
    "        height_ori, width_ori = img.shape[:2]\n",
    "        img = np.ascontiguousarray(img)\n",
    "        #bboxes_0, confis_0,classes_0 = predict(model_0, img, size=IMG_SIZE, augment=AUGMENT)\n",
    "        #bboxes_1, confis_1,classes_1 = predict(model_1, img, size=IMG_SIZE, augment=AUGMENT)\n",
    "        #bboxes_2, confis_2,classes_2 = predict(model_2, img, size=IMG_SIZE, augment=AUGMENT)\n",
    "        #bboxes_3, confis_3,classes_3 = predict(model_3, img, size=IMG_SIZE, augment=AUGMENT)\n",
    "        #bboxes_4, confis_4,classes_4 = predict(model_4, img, size=IMG_SIZE, augment=AUGMENT)\n",
    "        \n",
    "        \n",
    "        #mmdet inference\n",
    "        #-------------------------\n",
    "        bboxes_5, confis_5,classes_5 = predict_mmdet(model_cascade_0,file_path)\n",
    "        \n",
    "        \n",
    "        #-------------------------\n",
    "        bboxes = [bboxes_5]#[bboxes_0,bboxes_1,bboxes_2,bboxes_3,bboxes_4]\n",
    "        confis = [confis_5]#[confis_0,confis_1,confis_2,confis_3,confis_4]\n",
    "        classes = [classes_5]#[classes_0,classes_1,classes_2,classes_3,classes_4]\n",
    "        bboxes,confis,classes = run_wbf(bboxes, confis, classes, iou_thr=0.5,weights=[1,1,1,1,1])\n",
    "        \n",
    "        if len(bboxes) != 0:\n",
    "            for i in range(len(bboxes)):\n",
    "                \n",
    "                cur_box = bboxes[i]\n",
    "                cur_box[0] *= float(width_ori)\n",
    "                cur_box[1] *= float(height_ori)\n",
    "                cur_box[2] *= float(width_ori)\n",
    "                cur_box[3] *= float(height_ori)\n",
    "                #cls part-------------\n",
    "                #cls_pred = clasifier_part(img,cur_box,model_cls)\n",
    "                xmin = cur_box[0]\n",
    "                ymin = cur_box[1] \n",
    "                width= cur_box[2] - cur_box[0]\n",
    "                height= cur_box[3] - cur_box[1]\n",
    "                cur_box_xywh = [int(xmin),int(ymin),int(width),int(height)]\n",
    "                cur_confi = float('%.4f' % confis[i])\n",
    "                cur_cls = classes[i]#cls_pred#\n",
    "                \n",
    "                cv2.rectangle(img, (int(cur_box[0]), int(cur_box[1])), (int(cur_box[2]), int(cur_box[3])), (255, 0, 0), 1)\n",
    "                cv2.putText(img,str(cur_cls),(int(cur_box[0]), int(cur_box[1])),cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 0), 2)\n",
    "                results.append({\"image_id\":cur_name,\"category_id\":int(cur_cls),\"bbox\":cur_box_xywh,\"score\":cur_confi})\n",
    "        cv2.imwrite(f'../data/plot_result/{cur_name}.png',img)\n",
    "        \n",
    "    with open('sub.json', 'w') as outfile:\n",
    "        json.dump(results, outfile)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
